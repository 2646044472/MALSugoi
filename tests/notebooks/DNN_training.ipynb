{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5960edbe",
   "metadata": {},
   "source": [
    "### Process data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4fecc1-f828-4942-98e8-f381073e9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a451aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# run this block once only\n",
    "project_root = os.path.abspath(\"../../\")  # 根据文件层级调整路径\n",
    "os.chdir(project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14992fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "anime_data = pd.read_csv('./data/anime_info/anime_data.csv')\n",
    "user_ratings = pd.read_csv('./data/user_animelist/anime_info.csv')\n",
    "\n",
    "# 清理 anime_data 数据\n",
    "anime_data[\"popularity\"] = anime_data[\"popularity\"].str.replace(\"#\", \"\").fillna(0).astype(int)\n",
    "anime_data[\"members\"] = anime_data[\"members\"].str.replace(\",\", \"\").fillna(0).astype(float).astype(int)\n",
    "anime_data[\"favorites\"] = anime_data[\"favorites\"].str.replace(\",\", \"\").fillna(0).astype(float).astype(int)\n",
    "\n",
    "# 为番剧分配唯一 ID\n",
    "anime_data[\"anime_id\"] = anime_data.index\n",
    "\n",
    "# 处理 genres 列，将每个类别分配一个整数 ID\n",
    "all_genres = set(g for genre_list in anime_data[\"genres\"].dropna() for g in genre_list.split(\", \"))\n",
    "genre_to_id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "anime_data[\"genre_ids\"] = anime_data[\"genres\"].apply(\n",
    "    lambda x: [genre_to_id[g] for g in x.split(\", \")] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "# 填充缺失值\n",
    "anime_data = anime_data.fillna({\n",
    "    \"popularity\": 0,\n",
    "    \"members\": 0,\n",
    "    \"favorites\": 0,\n",
    "    \"genres\": \"\"\n",
    "})\n",
    "\n",
    "# 清理 user_ratings 数据\n",
    "user_ratings = user_ratings[user_ratings[\"rating\"] != \"-\"]\n",
    "user_ratings[\"rating\"] = user_ratings[\"rating\"].astype(float)\n",
    "\n",
    "# 为用户分配唯一 ID\n",
    "user_to_id = {user: idx for idx, user in enumerate(user_ratings[\"username\"].unique())}\n",
    "user_ratings[\"user_id\"] = user_ratings[\"username\"].map(user_to_id)\n",
    "\n",
    "# 为番剧名映射对应的 anime_id\n",
    "anime_name_to_id = dict(zip(anime_data[\"title\"], anime_data[\"anime_id\"]))\n",
    "user_ratings[\"anime_id\"] = user_ratings[\"anime\"].map(anime_name_to_id)\n",
    "\n",
    "# 移除无效的番剧映射\n",
    "user_ratings = user_ratings.dropna(subset=[\"anime_id\"])\n",
    "user_ratings[\"anime_id\"] = user_ratings[\"anime_id\"].astype(int)\n",
    "\n",
    "# 准备模型输入\n",
    "model_input = user_ratings.merge(anime_data, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "# 检查并移除缺失值\n",
    "model_input = model_input.dropna()\n",
    "\n",
    "# 确保 `genre_ids` 无空值\n",
    "model_input[\"genre_ids\"] = model_input[\"genre_ids\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# 提取用户特征和番剧特征\n",
    "X = {\n",
    "    \"user_id\": model_input[\"user_id\"].values,\n",
    "    \"anime_id\": model_input[\"anime_id\"].values,\n",
    "    \"anime_meta\": model_input[[\"score\", \"members\", \"favorites\"]].values,\n",
    "    \"genre_id\": model_input[\"genre_ids\"].values\n",
    "}\n",
    "\n",
    "# 提取目标变量 (评分)\n",
    "y = model_input[\"rating\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a754a373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': array([  1,   1,   1, ..., 559, 559, 559]),\n",
       " 'anime_id': array([2881, 3445, 4462, ...,  295, 3316, 2478]),\n",
       " 'anime_meta': array([[7.28000e+00, 3.91550e+04, 4.70000e+01],\n",
       "        [7.18000e+00, 1.30833e+05, 2.37000e+02],\n",
       "        [6.97000e+00, 1.20138e+05, 1.56000e+02],\n",
       "        ...,\n",
       "        [8.28000e+00, 3.02886e+05, 2.70500e+03],\n",
       "        [7.20000e+00, 1.19583e+05, 5.73000e+02],\n",
       "        [7.36000e+00, 6.52970e+04, 3.19000e+02]]),\n",
       " 'genre_id': array([list([16, 2, 49, 46, 22, 39, 75, 64]), list([16, 45, 35, 49, 64]),\n",
       "        list([16, 45, 49, 64]), ..., list([21, 2, 35, 46]),\n",
       "        list([16, 35, 56, 58, 47]), list([16, 36, 56, 38, 47])],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c519b1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 9., 3., ..., 8., 7., 7.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7330f44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15420"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bccf195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users = 528\n",
      "num_animes = 973\n",
      "num_genres = 72\n",
      "embed_dim = 32\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# 计算用户总数\n",
    "num_users = len(set(X[\"user_id\"]))\n",
    "\n",
    "# 计算动漫总数\n",
    "num_animes = len(set(X[\"anime_id\"]))\n",
    "\n",
    "# 计算类型总数\n",
    "num_genres = len(set(chain.from_iterable(X[\"genre_id\"])))\n",
    "\n",
    "# 嵌入维度（超参数，可以调整）\n",
    "embed_dim = 32  # 或者其他合适的值，如 64\n",
    "\n",
    "print(f\"num_users = {num_users}\")\n",
    "print(f\"num_animes = {num_animes}\")\n",
    "print(f\"num_genres = {num_genres}\")\n",
    "print(f\"embed_dim = {embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c30ca",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "194be0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume X and y are already defined\n",
    "unique_user_ids = np.unique(X[\"user_id\"])\n",
    "unique_anime_ids = np.unique(X[\"anime_id\"])\n",
    "\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "anime_id_to_idx = {anime_id: idx for idx, anime_id in enumerate(unique_anime_ids)}\n",
    "\n",
    "X[\"user_id\"] = np.array([user_id_to_idx[uid] for uid in X[\"user_id\"]])\n",
    "X[\"anime_id\"] = np.array([anime_id_to_idx[aid] for aid in X[\"anime_id\"]])\n",
    "\n",
    "num_users = len(unique_user_ids)\n",
    "num_anime = len(unique_anime_ids)\n",
    "max_genre_id = max([max(genre) for genre in X[\"genre_id\"]])\n",
    "num_genres = max_genre_id + 1\n",
    "\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.user_ids = torch.tensor(X[\"user_id\"], dtype=torch.long)\n",
    "        self.anime_ids = torch.tensor(X[\"anime_id\"], dtype=torch.long)\n",
    "        self.anime_meta = torch.tensor(X[\"anime_meta\"], dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.genre_ids = [torch.tensor(genre, dtype=torch.long) for genre in X[\"genre_id\"]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.user_ids[idx],\n",
    "            self.anime_ids[idx],\n",
    "            self.anime_meta[idx],\n",
    "            self.genre_ids[idx],\n",
    "            self.y[idx]\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    user_ids = torch.stack([item[0] for item in batch])\n",
    "    anime_ids = torch.stack([item[1] for item in batch])\n",
    "    anime_meta = torch.stack([item[2] for item in batch])\n",
    "    ratings = torch.stack([item[4] for item in batch])\n",
    "    \n",
    "    genre_ids = [item[3] for item in batch]\n",
    "    max_len = max([g.shape[0] for g in genre_ids])\n",
    "    padded_genre_ids = torch.full((len(batch), max_len), fill_value=-1, dtype=torch.long)\n",
    "    for i, g in enumerate(genre_ids):\n",
    "        padded_genre_ids[i, :g.shape[0]] = g\n",
    "    \n",
    "    return user_ids, anime_ids, anime_meta, padded_genre_ids, ratings\n",
    "\n",
    "dataset = AnimeDataset(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff123717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 12336\n",
      "Test dataset size: 3084\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 数据集划分比例\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# 数据集总大小\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# 计算训练集和测试集大小\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# 随机划分数据集\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 创建训练集和测试集的 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45190b19",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29fc11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AnimeRecommendationModel(nn.Module):\n",
    "    def __init__(self, num_users, num_animes, num_genres, embed_dim=64):\n",
    "        super(AnimeRecommendationModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers for user and anime\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=embed_dim)\n",
    "        self.anime_embedding = nn.Embedding(num_embeddings=num_animes, embedding_dim=embed_dim)\n",
    "        \n",
    "        # Linear layer for anime meta features\n",
    "        self.anime_meta_fc = nn.Linear(3, 16)  # 处理动漫元数据\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(embed_dim * 2 + 16 + num_genres, 128)  # 第一层全连接\n",
    "        self.fc2 = nn.Linear(128, 64)  # 第二层全连接\n",
    "        self.output = nn.Linear(64, 1)  # 输出评分\n",
    "\n",
    "    def forward(self, user_id, anime_id, anime_meta, genre_ids):\n",
    "        # User embedding\n",
    "        user_embedded = self.user_embedding(user_id)\n",
    "        \n",
    "        # Anime embedding\n",
    "        anime_embedded = self.anime_embedding(anime_id)\n",
    "        \n",
    "        # Anime meta features\n",
    "        anime_meta_processed = F.relu(self.anime_meta_fc(anime_meta))\n",
    "        \n",
    "        # Genre processing (one-hot encoding and pooling)\n",
    "        mask = (genre_ids != -1)\n",
    "        genre_ids = genre_ids * mask\n",
    "        genre_embedded = F.one_hot(genre_ids, num_classes=num_genres).float()\n",
    "        genre_embedded = genre_embedded * mask.unsqueeze(-1)\n",
    "        genre_embedded = torch.mean(genre_embedded, dim=1)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        x = torch.cat([user_embedded, anime_embedded, anime_meta_processed, genre_embedded], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d772429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeRecommendationModel(num_users=num_users, num_animes=num_animes, num_genres=num_genres).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d943c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnimeRecommendationModel(\n",
      "  (user_embedding): Embedding(528, 64)\n",
      "  (anime_embedding): Embedding(973, 64)\n",
      "  (anime_meta_fc): Linear(in_features=3, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=220, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cd922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 393068.2491\n",
      "Epoch [2/100], Loss: 68.5266\n",
      "Epoch [3/100], Loss: 2455.5193\n",
      "Epoch [4/100], Loss: 124517.8724\n",
      "Epoch [5/100], Loss: 4192.9556\n",
      "Epoch [6/100], Loss: 90.2329\n",
      "Epoch [7/100], Loss: 64.2079\n",
      "Epoch [8/100], Loss: 5088.7036\n",
      "Epoch [9/100], Loss: 2969.5515\n",
      "Epoch [10/100], Loss: 38227.3479\n",
      "Epoch [11/100], Loss: 858.1166\n",
      "Epoch [12/100], Loss: 68.3244\n",
      "Epoch [13/100], Loss: 7104.4008\n",
      "Epoch [14/100], Loss: 4357.4459\n",
      "Epoch [15/100], Loss: 7031.1282\n",
      "Epoch [16/100], Loss: 120367.4816\n",
      "Epoch [17/100], Loss: 24.6030\n",
      "Epoch [18/100], Loss: 24.1746\n",
      "Epoch [19/100], Loss: 22.8505\n",
      "Epoch [20/100], Loss: 22.8234\n",
      "Epoch [21/100], Loss: 22.7108\n",
      "Epoch [22/100], Loss: 21.8290\n",
      "Epoch [23/100], Loss: 20.2231\n",
      "Epoch [24/100], Loss: 20.3541\n",
      "Epoch [25/100], Loss: 17.8030\n",
      "Epoch [26/100], Loss: 16.2849\n",
      "Epoch [27/100], Loss: 18.1154\n",
      "Epoch [28/100], Loss: 44.0035\n",
      "Epoch [29/100], Loss: 21.5900\n",
      "Epoch [30/100], Loss: 29.7406\n",
      "Epoch [31/100], Loss: 27.2587\n",
      "Epoch [32/100], Loss: 27.6242\n",
      "Epoch [33/100], Loss: 1033.4759\n",
      "Epoch [34/100], Loss: 283.8783\n",
      "Epoch [35/100], Loss: 81.7943\n",
      "Epoch [36/100], Loss: 27.4018\n",
      "Epoch [37/100], Loss: 37.0748\n",
      "Epoch [38/100], Loss: 4955.7504\n",
      "Epoch [39/100], Loss: 276.7758\n",
      "Epoch [40/100], Loss: 9.4004\n",
      "Epoch [41/100], Loss: 7.9911\n",
      "Epoch [42/100], Loss: 7.1240\n",
      "Epoch [43/100], Loss: 6.4973\n",
      "Epoch [44/100], Loss: 6.2940\n",
      "Epoch [45/100], Loss: 12.8876\n",
      "Epoch [46/100], Loss: 9.6481\n",
      "Epoch [47/100], Loss: 196.8191\n",
      "Epoch [48/100], Loss: 13.7328\n",
      "Epoch [49/100], Loss: 6.3483\n",
      "Epoch [50/100], Loss: 5.4959\n",
      "Epoch [51/100], Loss: 10.0859\n",
      "Epoch [52/100], Loss: 40.0065\n",
      "Epoch [53/100], Loss: 13.5440\n",
      "Epoch [54/100], Loss: 275.5604\n",
      "Epoch [55/100], Loss: 8.0599\n",
      "Epoch [56/100], Loss: 6.7072\n",
      "Epoch [57/100], Loss: 4.5791\n",
      "Epoch [58/100], Loss: 90.7210\n",
      "Epoch [59/100], Loss: 11.9958\n",
      "Epoch [60/100], Loss: 9.6959\n",
      "Epoch [61/100], Loss: 7.8458\n",
      "Epoch [62/100], Loss: 91.1478\n",
      "Epoch [63/100], Loss: 6.8717\n",
      "Epoch [64/100], Loss: 6.5640\n",
      "Epoch [65/100], Loss: 106.6853\n",
      "Epoch [66/100], Loss: 5.4099\n",
      "Epoch [67/100], Loss: 4.2453\n",
      "Epoch [68/100], Loss: 4.0222\n",
      "Epoch [69/100], Loss: 3.7437\n",
      "Epoch [70/100], Loss: 4.3153\n",
      "Epoch [71/100], Loss: 50.7864\n",
      "Epoch [72/100], Loss: 3.9789\n",
      "Epoch [73/100], Loss: 3.7443\n",
      "Epoch [74/100], Loss: 5.2395\n",
      "Epoch [75/100], Loss: 7.0447\n",
      "Epoch [76/100], Loss: 3.1398\n",
      "Epoch [77/100], Loss: 3.3993\n",
      "Epoch [78/100], Loss: 2.6921\n",
      "Epoch [79/100], Loss: 2.9219\n",
      "Epoch [80/100], Loss: 4.0628\n",
      "Epoch [81/100], Loss: 4.4284\n",
      "Epoch [82/100], Loss: 3.9859\n",
      "Epoch [83/100], Loss: 4.0393\n",
      "Epoch [84/100], Loss: 3.0618\n",
      "Epoch [85/100], Loss: 2.6781\n",
      "Epoch [86/100], Loss: 3.7616\n",
      "Epoch [87/100], Loss: 3.0902\n",
      "Epoch [88/100], Loss: 2.4290\n",
      "Epoch [89/100], Loss: 2.2960\n",
      "Epoch [90/100], Loss: 2.4723\n",
      "Epoch [91/100], Loss: 3.3285\n",
      "Epoch [92/100], Loss: 2.5897\n",
      "Epoch [93/100], Loss: 4324.2786\n",
      "Epoch [94/100], Loss: 18.7425\n",
      "Epoch [95/100], Loss: 18.3590\n",
      "Epoch [96/100], Loss: 18.5227\n",
      "Epoch [97/100], Loss: 17.7340\n",
      "Epoch [98/100], Loss: 16.8424\n",
      "Epoch [99/100], Loss: 15.5090\n",
      "Epoch [100/100], Loss: 14.4620\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        user_id, anime_id, anime_meta, genre_ids, ratings = batch\n",
    "        user_id = user_id.to(device)\n",
    "        anime_id = anime_id.to(device)\n",
    "        anime_meta = anime_meta.to(device)\n",
    "        genre_ids = genre_ids.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "        \n",
    "        outputs = model(user_id, anime_id, anime_meta, genre_ids)\n",
    "        loss = criterion(outputs.squeeze(), ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f25c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for batch in test_loader:\n",
    "            user_id, anime_id, anime_meta, genre_ids, ratings = batch\n",
    "            user_id = user_id.to(device)\n",
    "            anime_id = anime_id.to(device)\n",
    "            anime_meta = anime_meta.to(device)\n",
    "            genre_ids = genre_ids.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            # 模型预测\n",
    "            outputs = model(user_id, anime_id, anime_meta, genre_ids)\n",
    "            loss = criterion(outputs.squeeze(), ratings)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # 保存预测值和真实值，用于后续计算指标\n",
    "            all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            all_targets.extend(ratings.cpu().numpy())\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # 转换为 NumPy 数组\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # 计算其他评估指标\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))  # 平均绝对误差\n",
    "    mse = np.mean((all_predictions - all_targets)**2)     # 均方误差\n",
    "    r2 = 1 - (np.sum((all_targets - all_predictions)**2) / np.sum((all_targets - np.mean(all_targets))**2))  # R²\n",
    "\n",
    "    return avg_loss, mae, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2b81ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 15.6292\n",
      "Mean Absolute Error (MAE): 3.5041\n",
      "Mean Squared Error (MSE): 15.6238\n",
      "R² Score: -5.8245\n"
     ]
    }
   ],
   "source": [
    "# 训练完成后，评估模型\n",
    "test_loss, mae, mse, r2 = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e654942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MALSugoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
